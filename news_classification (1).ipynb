{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Article Classification using PyTorch\n",
    "\n",
    "A text classification model to automatically categorize news articles into four classes: World, Sports, Business, and Sci/Tech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%pip install pandas numpy==1.26.4 seaborn==0.9.0 matplotlib scikit-learn portalocker>=2.0.0 plotly\n",
    "%pip install torch==2.3.0+cpu torchdata==0.9.0+cpu torchtext==0.18.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(COST, ACC):\n",
    "    \"\"\"Plot training loss and accuracy\"\"\"\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('epoch', color=color)\n",
    "    ax1.set_ylabel('total loss', color=color)\n",
    "    ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('accuracy', color=color)\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', color=color)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG_NEWS dataset\n",
    "train_iter = iter(AG_NEWS(split=\"train\"))\n",
    "y, text = next(train_iter)\n",
    "print(f\"Sample: {y}, {text}\")\n",
    "\n",
    "# Label mapping\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "print(f\"Label: {ag_news_label[y]}\")\n",
    "\n",
    "# Get number of classes\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "print(f\"Number of classes: {num_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize train_iter\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "\n",
    "# Define tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text.lower())\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample token indices: {vocab(['age', 'hello'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Create 95/5 train/validation split\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "# Verify data\n",
    "label, text, offsets = next(iter(valid_dataloader))\n",
    "print(f\"Batch shapes - Labels: {label.shape}, Text: {text.shape}, Offsets: {offsets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 64\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "print(model)\n",
    "\n",
    "# Verify output shape\n",
    "predicted_label = model(text, offsets)\n",
    "print(f\"Output shape: {predicted_label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    \"\"\"Predict the category of a text article\"\"\"\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return ag_news_label[output.argmax(1).item() + 1]\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"Evaluate model accuracy on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "# Test before training\n",
    "print(f\"Accuracy before training: {evaluate(test_dataloader):.4f}\")\n",
    "print(f\"Sample prediction: {predict('I like sports', text_pipeline)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.1\n",
    "EPOCHS = 10\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_loss_list = []\n",
    "acc_epoch = []\n",
    "acc_old = 0\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    model.train()\n",
    "    cum_loss = 0\n",
    "    \n",
    "    for idx, (label, text, offsets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "    cum_loss_list.append(cum_loss)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    acc_epoch.append(accu_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss = {cum_loss:.4f}, Validation Accuracy = {accu_val:.4f}\")\n",
    "\n",
    "    if accu_val > acc_old:\n",
    "        acc_old = accu_val\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"  -> Model saved with accuracy: {accu_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cum_loss_list, acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = evaluate(test_dataloader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings from validation batch\n",
    "batch = next(iter(valid_dataloader))\n",
    "label, text, offsets = batch\n",
    "text = text.to(device)\n",
    "offsets = offsets.to(device)\n",
    "\n",
    "embedded = model.embedding(text, offsets)\n",
    "embeddings_numpy = embedded.detach().cpu().numpy()\n",
    "\n",
    "# Perform t-SNE\n",
    "X_embedded_3d = TSNE(n_components=3).fit_transform(embeddings_numpy)\n",
    "\n",
    "# Create 3D scatter plot\n",
    "trace = go.Scatter3d(\n",
    "    x=X_embedded_3d[:, 0],\n",
    "    y=X_embedded_3d[:, 1],\n",
    "    z=X_embedded_3d[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=label.numpy(),\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"3D t-SNE Visualization of Embeddings\",\n",
    "    scene=dict(\n",
    "        xaxis_title='Dimension 1',\n",
    "        yaxis_title='Dimension 2',\n",
    "        zaxis_title='Dimension 3'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Sample Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"Canada navigated a stiff test against the Republic of Ireland on a rain soaked evening in Perth, \n",
    "coming from behind to claim a vital 2-1 victory at the Women's World Cup. Katie McCabe opened the scoring \n",
    "with an incredible Olimpico goal – scoring straight from a corner kick – as her corner flew straight over \n",
    "the despairing Canada goalkeeper Kailen Sheridan at Perth Rectangular Stadium in Australia.\"\"\"\n",
    "\n",
    "result = predict(article, text_pipeline)\n",
    "print(f\"Article: {article}\\n\")\n",
    "print(f\"Predicted Category: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the best saved model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction on Multiple Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles = [\n",
    "    \"International talks have made significant headway with the signing of a climate accord.\",\n",
    "    \"In a stunning upset, the underdog team won the national title.\",\n",
    "    \"Market analysts are optimistic as the tech startup's stock prices soared.\",\n",
    "    \"A recent study suggests that a new drug has shown promise in treating Alzheimer's disease.\",\n",
    "    \"The sports world is in shock as a legendary player announces their retirement.\"\n",
    "]\n",
    "\n",
    "for i, article in enumerate(new_articles, start=1):\n",
    "    prediction = predict(article, text_pipeline)\n",
    "    print(f\"Article {i}: {prediction}\")\n",
    "    print(f\"  Text: {article[:80]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
